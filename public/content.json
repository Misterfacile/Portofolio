{
  "meta": {
    "siteTitle": "Paul GUAN's Portfolio",
    "siteDescription": "Junior engineer with a passion for AI/DATA (NLP, Machine Learning, Deep Learning), with a solid foundation in software and web development.",
    "author": "Paul GUAN",
    "social": {
      "github": "https://github.com/Misterfacile",
      "linkedin": "https://www.linkedin.com/in/paul-guan/"
    }
  },
  "hero": {
    "headline": "Hey, I'm Paul GUAN and Welcome to my portfolio",
    "subheadline": "",
    "profileImage": "/images/pic03.jpg",
    "ctaPrimaryText": "View Projects",
    "ctaSecondaryText": "Contact Me"
  },
  "about": {
    "bio": "Junior Engineer in AI, Data, and Software Development, passionate about building intelligent, scalable systems. Currently seeking opportunities as a Data Engineer, AI Engineer, or Software Engineer to contribute to innovative, real-world projects. Driven by curiosity, adaptability, and a constant desire to learn and create meaningful impact.",
    "highlights": [
      "AI & Gen AI & LLMs & Deep Learning & NLP",
      "ML Engineering & MLOps",
      "Big Data & Data Engineering & Analytics",
      "Sofware Engineering / Fullstack development"
    ],
    "cvUrl": "/PaulGuan_CV.pdf"
  },
  "experience": [
    {
      "company": "SES Satellite",
      "role": "Data Engineering Intern - Data Services",
      "location": "Betzdorf - Luxembourg",
      "from": "2025-09",
      "to": "present",
      "logo": "/images/experiences/SES.png",
      "bullets": [
        "Designed and implemented ETL pipelines for large-scale data retrieval and transformation using Python and PySpark",
        "Implemented end-to-end data pipelines in Azure Databricks, ensuring scalability and reliability",
        "Defined and standardized data management processes leveraging the Medallion Architecture and Unified Metric Model (UMM)",
        "Authored comprehensive data workflow documentation using AI-powered tools, achieving 300% faster delivery while improving process visibility, traceability, and data governance",
        "Reviewed and optimized existing codebases across SES Data Services projects, enhancing 10%-20% performance and maintainability"
      ],
      "tech": ["SQL", "Python", "PySpark", "Kafka", "Delta Lake", "Azure Data Factory", "Azure Databricks", "Github Copilot", "MCP", "Azure"]
    },
    {
      "company": "SII",
      "role": "Sofware Engineer - AI",
      "location": "Vélizy-Villacoublay - Ile de France - France ",
      "from": "2025-02",
      "to": "2025-08",
      "logo": "/images/experiences/SII/SII.png",
      "bullets": [
        "Developed defence-grade software and radar systems enhanced by AI technologies in C++ using Qt framework",
        "Created and optimized targeted prompts for LLMs, including GitHub Copilot, resulting in a 50% productivity increase", 
        "Researched and evaluated the effectiveness of AI in software engineering workflows and productivity through multiple use cases and KPI-driven analyses",
        "Conducted continuous technology watch on AI innovations, best practices, and industrial software development methodologies",
        "AGILE Project"
      ],
      "tech": ["C++", "Qt", "Gitlab CRI", "MCP", "Github Copilot",  "Qt Creator"],
      "images": ["/images/experiences/SII/SII-1.jpg", "/images/experiences/SII/SII-2.jpg", "/images/experiences/SII/SII-3.jpg", "/images/experiences/SII/SII-4.jpg"]
    },
    {
      "company": "Brainsonic",
      "role": "Fullstack Developer",
      "location": "Paris - France ",
      "from": "2023-09",
      "to": "2024-02",
      "logo": "/images/experiences/Brainsonic/Brainsonic.png",
      "bullets": [
        "Designed, enhanced, and developed innovative web features for internal platforms and client websites using Symfony and WordPress",
        "Partnered with design, product, and engineering teams to overcome technical challenges and deliver client-approved solutions meeting 100% satisfaction goals",
        "Built Web Applications and Online Games from the ground up, ensuring optimal performance and scalability",
        "Enhanced backend architecture RESTAPI / ORM and analytics dashboards to optimize data visualization performance and website management workflows, resulting in a 20–30% efficiency gain"
      ],
      "tech": ["Web 3.0", "HTML", "CSS", "JS", "PHP", "PostgreSQL", "Heroku", "WordPress"],
      "images": ["/images/experiences/Brainsonic/01.png", "/images/experiences/Brainsonic/02.png", "/images/experiences/Brainsonic/03.png", "/images/experiences/Brainsonic/04.png", "/images/experiences/Brainsonic/05.png"]
    }
  ],
  "education": [
    {
      "school": "Epita, Engineering school in Computer Intelligence",
      "program": "MSc Computer Science IA - Data",
      "from": "2022",
      "to": "2025",
      "details": [
        "Machine Learning",
        "Deep Learning",
        "Natural Language Processing",
        "Computer Vision",
        "Reinforcement Learning",
        "Prompt Engineering",
        "DataOps",
        "MLOps",
        "MCP/RAG/CAG", 
        "AI/Gen-AI/LLM",
        "API Integration",
        "RESTful APIs",
        "Probabilistic Graphical Models",
        "Algorithms & Data Structures",
        "Data Engineering/Analytics",
        "Big Data",
        "Software Development",
        "DevOps",
        "Ethics AI",
        "Project management",
        "Multiple AI, Data, and Software projects applied to real-world use cases with multiple technologies"
      ],
      "badge": "/images/educations/EPITA.jpg"
    },
    {
      "school": "University of Paris Cite Descartes",
      "program": "BSc Computer Science",
      "from": "2019",
      "to": "2022",
      "details": [
        "Fundamentals of Computer Science and core programming concepts",
        "Algorithms & Data Structures",
        "Object-Oriented Programming (Java, Python, C++)",
        "Database Design & SQL",
        "Web Development (frontend & backend)",
        "Image Recognition & Computer Vision projects",
        "Development of Software",
        "Group projects and collaborative software development"
      ],
      "badge": "/images/educations/UP.png"
    }
  ],
  "skills": {
    "languages": ["Python", "Java", "C", "C++", "SQL", "JavaScript", "HTML", "CSS", "PHP", "Shell"],
    "frameworks_libraries": ["Symfony", "React", "Node.Js", "Qt", "Pyspark", "Kafka", "FastAPI", "Streamlit", "PyTorch", "TensorFlow", "scikit-learn", "HuggingFace", "Keras", "SparkML", "Numpy", "Pandas"],
    "databases": ["PostgreSQL", "Cassandra", "MongoDB", "DynamoDB", "Neoj4"],
    "devops": ["Docker", "Kubernetes", "Github Action"],
    "cloud": ["AWS", "Azure", "Databricks"],
    "tools": ["Jupyter", "Kaggle", "Google Colabs",  "Git", "VIM", "SVN", "Linux/MACOS/Windows", "Shopify", "Tiktok Shop"],
    "softskills": ["Curiosity", "Autonomy",  "Communication", "Flexibility",  "Adaptability", "Problem Solving"]
  },
  "certifications": [
    {
      "title": "IBM AI Engineering Professional Certificate (V3)",
      "issuer": "IBM",
      "date": "2025-11",
      "verifyUrl": "https://www.credly.com/badges/bdb0f6a6-071f-4679-8330-d374d4f9e07f/public_url",
      "logo": "/images/certifications/IBM.webp"
    },
    {
      "title": "IBM Data Engineering Professional Certificate",
      "issuer": "IBM",
      "date": "2025-10",
      "verifyUrl": "https://www.credly.com/badges/14b64f4e-2615-45b7-ab65-051f59c209b4/linked_in_profile",
      "logo": "/images/certifications/IBM.webp"
    },
    {
      "title": "Build Your Generative AI Productivity Skills with Microsoft and LinkedIn",
      "issuer": "Microsoft",
      "date": "2025-10",
      "verifyUrl": "https://www.linkedin.com/learning/certificates/85344ca1a4896c068c288820ecd9b3fb1b0ee6e4e14270ef393ec74f66884f1e?u=133596794",
      "logo": "/images/certifications/Microsoft.png"
    },
    {
      "title": "Intro to Generative AI: A Beginner's Primer on Core Concepts",
      "issuer": "Google Cloud",
      "date": "2025-10",
      "verifyUrl": "https://www.coursera.org/account/accomplishments/specialization/N6JQ1PEZTDVM",
      "logo": "/images/certifications/Google_Cloud.png"
    },
    {
      "title": "Others certificates",
      "issuer": "Credly",
      "verifyUrl": "https://www.credly.com/users/paul-guan.056bec01/badges#credly",
      "logo": "/images/certifications/Credly.png"
    }
    
  ],
  "projects": [
    {
      "title": "SaaS - TokManage, Tiktok Shop Live Management - Freelance work",
      "startDate": "2025-08",
      "endDate": "Present",
      "images": ["/images/projects/TokManage/TokManage-1.png",
      
      "/images/projects/TokManage/TokManage-2.png",
      "/images/projects/TokManage/TokManage-3.png",
      "/images/projects/TokManage/TokManage-4.png",
      "/images/projects/TokManage/TokManage-5.png",
      "/images/projects/TokManage/TokManage-6.png",
      "/images/projects/TokManage/TokManage-7.png"
      ],
      "description": "TokManage is a web platform designed to automate order and logistics management by integrating directly with TikTok Shop and Shopify APIs. It simplifies the export and synchronization of shipping files (CSV/Excel) while offering real-time tracking and visibility across the entire fulfillment process. The platform’s backend, built with FastAPI and SQLAlchemy, works seamlessly with a Streamlit-based dashboard that enables automatic carrier file generation, live order updates, and instant data visualization. An intelligent algorithm matches Shopify orders with tracking numbers, reducing manual processing by 90% and significantly improving operational efficiency. The system also automates bonus and work-hour calculations, providing transparent, performance-based rewards for logistics employees. Deployed in a containerized architecture using Docker and Kubernetes (kind), TokManage supports one-click deployment for both local and cloud environments, ensuring consistency and fast onboarding. Continuous integration and deployment are fully automated through GitHub Actions and Kubernetes manifests, allowing smooth, scalable, and reliable updates to the platform.",
      "tech": ["Python", "Streamlit", "FastAPI", "PostgreSQL", "Docker", "Kubernetes", "Shopify API"],
      "links": {
      },
      "tags": ["Software", "SaaS", "Data Engineering"]
    },
    {
      "title": "Vinted Scrapper - Predictor Seller",
      "startDate": "2024-12",
      "endDate": "2024-01",
      "images": ["/images/projects/VintedScrapper/VintedScrapper-1.png", "/images/projects/VintedScrapper/VintedScrapper-2.png", "/images/projects/VintedScrapper/VintedScrapper-3.png", "/images/projects/VintedScrapper/VintedScrapper-4.png", "/images/projects/VintedScrapper/VintedScrapper-5.png", "/images/projects/VintedScrapper/VintedScrapper-6.png", "/images/projects/VintedScrapper/VintedScrapper-7.png", "/images/projects/VintedScrapper/VintedScrapper-8.png"],
      "description": "Developed a data-driven analytics and prediction pipeline based on e-commerce data. The project involved structured data collection through web scraping with Python (BeautifulSoup, Scrapy, Selenium) while handling anti-DDOS protection mechanisms. Scraped data was cleansed, transformed, and stored in formats such as Pandas DataFrames, CSV/Excel, and integrated into a Cassandra database for large-scale analysis. Data was then labeled and pre-processed for Machine Learning tasks. An interactive Streamlit dashboard with dynamic filters enabled targeted analysis and visualization of trends, pricing, and user behavior. Additionally, a predictive model leveraging CamemBERT (Hugging Face) was developed to estimate whether an item would sell within a given timeframe, achieving an accuracy of ~70%.",
      "tech": ["Python", "Apache Cassandra", "Streamlit", "Docker"],
      "links": {
      },
      "tags": ["Software", "Deep Learning", "Data Engineering", "Machine Learning"]
    },
    {
      "title": "Copilo.sh - WEB",
      "startDate": "2024-12",
      "endDate": "2024-12",
      "images": ["/images/projects/Copilosh/Architecture CopiloSH Web.png", "/images/projects/Copilosh/Copilosh.png"],
      "description": "Copilo.sh is a Python project designed as a wrapper function that can be added to a .bashrc or .zshrc file. It monitors terminal activity, catching errors (non-zero exit codes) and forwarding them to a FastAPI local server running a lightweight language model (LM) on CPU, which generates helpful responses to resolve the errors. The system was later deployed on a Microsoft Azure VM as a canary deployment, making the API accessible remotely without requiring local execution.",
      "tech": ["Python", "React", "Docker", "FastAPI", "Microsoft Azure"],
      "links": {
        "repo": "https://github.com/Misterfacile/CopiloSH",
        "live": "https://drive.google.com/file/d/1TMz7-njys61Gy3mExDFGapakw1ecEvZM/view?usp=sharing"
      },
      "tags": ["Software", "NLP", "Cloud"]
    },
        {
      "title": "Big Data Pipeline - BIG CACA",
      "startDate": "2024-11",
      "endDate": "2024-11",
      "images": ["/images/projects/BigCACA/BigCACA-1.png", "/images/projects/BigCACA/BigCACA-2.png", "/images/projects/BigCACA/BigCACA-3.png", "/images/projects/BigCACA/BigCACA-4.png"],
      "description": "The goal of this project was to analyze customer data using Apache Spark to extract insights into customer behavior and transaction trends. We processed and analyzed a dataset containing customer profiles and transactions, applying both analytics and machine learning techniques to uncover deeper patterns. The data pipeline was deployed on AWS, leveraging S3, EMR, Step Functions, and EventBridge for scalable processing. Infrastructure was automated with Terraform, and the preprocessed data was integrated into Amazon DocumentDB for efficient storage and querying",
      "tech": ["Python", "PySpark", "AWS S3/StepFunction/EventBridge/DocumentDB", "Terraform"],
      "links": {
      },
      "tags": ["Big Data", "Data Engineering", "Data Analytics", "Machine Learning"]
    },
    {
      "title": "Recommender Jobs Backend - Company Matchly",
      "startDate": "2024-09",
      "endDate": "2024-12",
      "images": ["/images/projects/Matchly/Matchly-1.jpg"],
      "description": "Implemented a job recommendation solution at Matchly, designed for both clients and recruiters. The project involved extracting and structuring data from job description sheets, followed by the determination of soft skills scores based on a predefined set of characteristics. We worked on the continuous improvement of supervised and unsupervised NLP models to enhance content quality, leveraging BERT and BIRD models for language understanding. A recommendation engine was built using K-Means clustering and cosine similarity, while a backend API was developed to deliver the system’s functionalities.",
      "tech": ["Python", "Scikit-learn", "FastAPI", "BERT/BIRD"],
      "links": {
      },
      "tags": ["NLP", "Recommender", "Deep Learning"]
    },
    {
      "title": "Movies Recommender",
      "startDate": "2024-06",
      "endDate": "2024-06",
      "images": ["/images/projects/MoviesRecommender/MoviesRecommender-1.png", "/images/projects/MoviesRecommender/MoviesRecommender-2.png", "/images/projects/MoviesRecommender/MoviesRecommender-3.png", "/images/projects/MoviesRecommender/MoviesRecommender-4.png", "/images/projects/MoviesRecommender/MoviesRecommender-5.png"],
      "description": "Developed a movie recommender system by combining collaborative filtering and content-based filtering techniques. The system was trained and evaluated using the MovieLens and IMDB datasets to deliver personalized movie recommendations",
      "tech": ["Python", "Pandas", "NumPy", "Scikit-learn"],
      "links": {
        "repo": "https://github.com/Misterfacile/Recommender_Movies"
      },
      "tags": ["NLP", "Recommender"]
    },
    {
      "title": "AWS - TermiCator",
      "startDate": "2024-06",
      "endDate": "2024-06",
      "images": ["/images/projects/TermiCator/TermiCator-1.png", "/images/projects/TermiCator/TermiCator-2.png", "/images/projects/TermiCator/TermiCator-3.png", "/images/projects/TermiCator/TermiCator-4.png", "/images/projects/TermiCator/TermiCator-5.png", "/images/projects/TermiCator/TermiCator-6.png"],
      "description": "Developed a Scala-based streaming alert system to process data from simulated IoT devices and notify users based on predefined criteria. The project focused on scalability and real-time data handling, leveraging AWS Kinesis, Firehose, S3, DynamoDB, and EMR for ingestion, storage, and processing. Infrastructure was provisioned with Terraform, ensuring reproducible and scalable deployment.",
      "tech": ["Scala", "AWS Kinesis", "AWS S3/Firehose/DynamoDB/EMR", "Terraform"],
      "links": {
      },
      "tags": ["Software", "Cloud", "Data Engineering"]
    },
    {
      "title": "Microsoft Azure - Weather Prediction",
      "startDate": "2024-06",
      "endDate": "2024-06",
      "images": ["/images/projects/WeatherAzur/WeatherAzur-1.png", "/images/projects/WeatherAzur/WeatherAzur-2.png"],
      "description": "Developed a simple weather prediction system on Microsoft Azure, leveraging Custom Vision and Azure Machine Learning for model training and deployment. The solution was integrated into an Azure Web App service, providing a lightweight interface for predictions.",
      "tech": ["Python", "Microsoft Azure", "Custom Vision", "Azure Machine Learning", "Azure Web App"],
      "links": {
      },
      "tags": ["Software", "Cloud", "Computer Vision"]
    },
    {
      "title": "Big Data - Stocks Dashboard",
      "startDate": "2024-04",
      "endDate": "2024-05",
      "images": ["/images/projects/PYDB/PYDB-1.png", "/images/projects/PYDB/PYDB-2.png", "/images/projects/PYDB/PYDB-3.png"],
      "description": "Developed a Python application to visualize company stock prices, including reading, cleaning, and storing large volumes of financial data. Leveraged NumPy, Pandas, and Matplotlib for data processing and analysis, and built an interactive dashboard with Dash to display stock trends and insights.",
      "tech": ["Python", "Pandas", "TimeSeriesDB", "Dash"],
      "links": {
        "repo": "https://github.com/Misterfacile/Big-Data-StocksDashboard"
      },
      "tags": ["Software", "Big Data", "Data Engineering"]
    },
    {
      "title": "CNN - Boat Image classification",
      "startDate": "2024-05",
      "endDate": "2024-05",
      "images": ["/images/projects/CNN-Navire/CNN-1.png", "/images/projects/CNN-Navire/CNN-2.png", "/images/projects/CNN-Navire/CNN-3.png"],
      "description": "Built a TensorFlow/Keras CNN to classify ship images into 10 categories, including data preprocessing, model architecture design, and training with tracked metrics (accuracy, loss, confusion matrix) to evaluate performance and guide iterations",
      "tech": ["Python", "TensorFlow", "Keras", "NumPy", "Pandas", "Matplotlib"],
      "links": {
      },
      "tags": ["Computer Vision", "Deep Learning"]
    },
    {
      "title": "NLP - Restaurant reviews classification",
      "startDate": "2024-04",
      "endDate": "2024-05",
      "images": ["/images/projects/NLP-Restaurant/NLP-Restaurant-1.png", "/images/projects/NLP-Restaurant/NLP-Restaurant-2.png", "/images/projects/NLP-Restaurant/NLP-Restaurant-3.png", "/images/projects/NLP-Restaurant/NLP-Restaurant-4.png", "/images/projects/NLP-Restaurant/NLP-Restaurant-5.png"],
      "description": "Developed a Python project for restaurant review classification using the SemEval-ABSA dataset. The workflow included dataset preprocessing with NLTK, regex, and byte-pair encoding for tokenization and normalization (stopword removal, lemmatization, lowercase conversion), followed by exploratory data statistics. Several predictive models were trained and compared, including Recurrent Neural Networks (RNNs) and Naive Bayes classifiers, to evaluate performance on sentiment and aspect-based classification tasks.",
      "tech": ["Python", "NLTK", "Scikit-learn", "Keras", "RNN", "Pandas", "Numpy", "Matplotlib", "Seaborn", "Torchj"],
      "links": {
        "repo": "https://github.com/Misterfacile/NLP-Classification-Review-Restaurant"
      },
      "tags": ["Machine Learning", "Deep Learning", "NLP"]
    },
    {
      "title": "Resolver Sudoku",
      "startDate": "2024-03",
      "endDate": "2024-04",
      "images": ["/images/projects/Resolver Sudoku/ResolverSudoku.jpg"],
      "description": "Developed a Sudoku solver in C# implementing the Dancing Links algorithm (DLX) to efficiently resolve exact cover problems and achieve fast puzzle solving",
      "tech": ["C#"],
      "links": {
      },
      "tags": ["Software"]
    },
    {
      "title": "UIMM - WorkAdventurer",
      "startDate": "2024-03",
      "endDate": "2024-04",
      "images": ["/images/projects/UIMM/UIMM-1.png"],
      "description": "Developed a web game for Brainsonic’s clients using the WorkAdventure framework and JavaScript, integrating custom maps, interactive quests, and branded assets to deliver an engaging, browser-based experience within virtual office spaces.",
      "tech": ["JavaScript"],
      "links": {
      },
      "tags": ["Software"]
    },
    {
      "title": "IDE Shrek",
      "startDate": "2023-06",
      "endDate": "2023-07",
      "images": ["/images/projects/Shrek-IDE/ShrekIDE-1.png", "/images/projects/Shrek-IDE/ShrekIDE-2.png", "/images/projects/Shrek-IDE/ShrekIDE-3.png"],
      "description": "Developed a Shrek-themed Integrated Development Environment (IDE) with a Java backend and a JavaScript/React frontend. The IDE supported Python code execution, core file operations (create, open, delete, move), and integrated Git and Maven functionalities. The final deliverable was containerized and deployed using Docker",
      "tech": ["Java, JavaScript, React, Git, Maven, Docker"],
      "links": {
      },
      "tags": ["Software"]
    },
    {
      "title": "Tiger Compiler",
      "startDate": "2023-03",
      "endDate": "2023-05",
      "images": ["/images/projects/Tiger Compiler/Tiger.jpg"],
      "description": "Implemented a compiler for Andrew W. Appel's Tiger language using C++",
      "tech": ["C++"],
      "links": {
      },
      "tags": ["Software"]
    },
    {
      "title": "BombermanWeb",
      "startDate": "2023-02",
      "endDate": "2023-02",
      "images": ["/images/projects/Bomberman/Bomberman.jpg"],
      "description": "Developed a Bomberman-inspired Web game in Java, featuring player movement, bomb placement, explosion mechanics and score.",
      "tech": ["Java"],
      "links": {
        "repo": "https://github.com/Misterfacile/BombermanWeb"
      },
      "tags": ["Software"]
    },
    {
      "title": "42sh",
      "startDate": "2023-01",
      "endDate": "2023-02",
      "images": ["/images/projects/42sh/42.png"],
      "description": "42sh is a project that involved building a POSIX-compliant shell from scratch, implementing core features of Unix shells such as command parsing, job control, redirections, and environment variable management, while ensuring full compliance with POSIX standards - Contact me for the link of the project",
      "tech": ["C"],
      "links": {
      },
      "tags": ["Software"]
    },
    {
      "title": "Genetic Pipeline to detect Cancer with the Institute Curie",
      "startDate": "2022-02",
      "endDate": "2022-02",
      "images": ["/images/projects/GeneticPipeline/Pipeline.png", "/images/projects/GeneticPipeline/ReadProcess.png"],
      "description": "This project, carried out in collaboration with the Institut Curie, focused on the recognition of cancer-causing genetic variants using advanced computational techniques. Developed a data processing pipeline with Snakemake, implemented neural networks in Keras for variant detection, and integrated bioinformatics tools such as Varsim, Samtools, and IGV for analysis.",
      "tech": ["Python", "SVN", "Varsim", "Samtools", "IGV", "Snakemake", "Keras"],
      "links": {
        "repo": "https://github.com/denliA/BioinformaticPipelineCNN"
      },
      "tags": ["Computer Vision", "Deep Learning"]
    },
    {
      "title": "Coins Recognition",
      "startDate": "2022-02",
      "endDate": "2022-02",
      "images": ["/images/projects/CoinsRecognition/coins.png"],
      "description": "Developed an image processing system in Python to recognize and segment coins. The project involved implementing noise reduction algorithms, coin detection techniques, and segmentation methods to accurately identify currency from images.",
      "tech": ["Python"],
      "links": {
        "repo": "https://github.com/Misterfacile/MoneyRecognition"
      },
      "tags": ["Computer Vision"]
    },
    {
      "title": "Competition of puck picking robot",
      "startDate": "2021-01",
      "endDate": "2021-05",
      "images": ["/images/projects/Robotique/robotique.png"],
      "description": "Developed a program in Java for a LEGO MINDSTORMS robot equipped with a color sensor and an ultrasonic sensor to autonomously retrieve pucks. The robot achieved a performance of 9 pucks in 1m50s on a 2m × 3m play field, as part of a collaborative team project with regular reporting, documentation, and a final defense.",
      "tech": ["SVN", "Java", "Lego MINDSTORMS", "Lejos"],
      "links": {
        "live": "https://drive.google.com/file/d/1QVoPxlcwQdrXL5oyKGyiN5QXJInVa7vp/view"
      },
      "tags": ["Robotics"]
    }
  ],
  "awards": [
    {
      "title": "AI DATA HACK - NLP Hackathon 2024 — 1st Place",
      "eventUrl": "https://www.defense.gouv.fr/sga/evenements/hackathon-ia-data-hack",
      "proofUrl": "https://www.linkedin.com/feed/update/urn:li:activity:7189667231117516801?originalSubdomain=fr",
      "date": "2024-04",
      "summary": "Developed a system to identify AI-generated text using Natural Language Processing (NLP) and Machine Learning models. The project focused on text preprocessing, feature engineering, and training classifiers to distinguish between human-written and AI-generated content, evaluating performance across multiple models.",
      "logo": "/images/Awards/AI_DATA_HACK_LOGO.jpg"
    }
  ],
  "contact": {
    "email": "pguan.pro@gmail.com",
    "successMessage": "Thanks for reaching out! I'll get back to you soon.",
    "errorMessage": "Something went wrong. Please try again or email me directly.",
    "privacyNote": "I'll only use your details to reply to your message."
  }
}
